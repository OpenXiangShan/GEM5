output header {{

inline uint32_t
calc_memsize(uint32_t rs, uint32_t re, uint32_t sew, uint32_t rVl) {
    uint32_t vend = std::min(rVl, re);
    uint32_t mem_size;
    if (rs < vend) {
        mem_size = (vend - rs) * sew / 8;
    } else {
        mem_size = 0;
    }
    return mem_size;
}

}};


def template VMemMacroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VMemTemplateMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VleConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    const uint32_t elem_num_per_vreg = VLEN / eew;
    // panic_if(nf * emul > 8, "invalid nf and emul");

    StaticInstPtr microop;
    VectorMicroInfo vmi;
    if (nf == 1) { // sequence load
        const uint32_t num_microops = emul;
        for (int i = 0; i < num_microops; ++i) {
            vmi.rs = i * elem_num_per_vreg;
            vmi.re = (i+1) * elem_num_per_vreg;
            vmi.microVd = VD + i;
            vmi.fn = 0;
            vmi.offset = (i * VLEN) / 8;
            if (vmi.microVd >= 32) {
                break;
            }
            microop = new %(class_name)sMicro(_machInst, i, vmi);
            microop->setDelayedCommit();
            microop->setFlag(IsLoad);
            microop->setNF(1);
            this->microops.push_back(microop);
        }
    } else {
        // segment load
        uint32_t vlmax = VLEN / sew * vflmul;
        for (int i=0; i < vlmax; i++) {
            for (int fn=0; fn < nf; fn++) {
                // baseAddr = Rs1 + (i*nf + fn) * eew / 8
                // (vd + fn * emul)<eew>[i] = mem<eew>[baseAddr];
                vmi.rs = i;
                vmi.re = i+1;
                vmi.fn = fn;
                vmi.microVd = elem_gen_idx(VD + fn * emul, i, eew/8);
                vmi.offset = (i*nf+fn) * eew / 8;
                microop = new %(class_name)sMicro(_machInst, i*(fn+1), vmi);

                // segment vl related
                microop->resetOpClass(VectorSegUnitStrideLoadOp);
                microop->setNF(nf);
                microop->setFlag(IsSegLoad);

                microop->setDelayedCommit();
                microop->setFlag(IsLoad);
                this->microops.push_back(microop);
            }
        }
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VleMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVd);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        SET_VL_SRC();
        SET_OLDDST_SRC();
        SET_VM_SRC();

        this->flags[IsVector] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;

};

}};

def template VleMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    Addr EA;
    // EA = Rs1 + vmi.offset;
    %(ea_code)s;

    uint32_t elem_num_per_vreg = VLEN / eew;
    uint32_t mem_size = calc_memsize(vmi.rs, vmi.re, eew, rVl);

    VM_REQUIRED();

    std::vector<bool> byte_enable(mem_size, true);
    Fault fault = xc->readMem(EA, Mem.as<uint8_t>(), mem_size, memAccessFlags,
                              byte_enable);
    if (fault != NoFault)
        return fault;

    COPY_OLD_VD();

    size_t ei;
    for (size_t i = 0; i < vmi.re - vmi.rs; i++) {
        uint32_t vdElemIdx = (vmi.rs % elem_num_per_vreg) + i;
        ei = i + vmi.rs;
        %(memacc_code)s;
    }

    %(op_wb)s;
    return fault;
}

}};

def template VloadMicroInitiateAcc {{

#if %(is_vecIndex)s

template<typename ElemType>
Fault
%(class_name)s<ElemType>::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const

#else

Fault
%(class_name)s::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const

#endif
{
#if %(is_vecIndex)s
    using vu = std::make_unsigned_t<ElemType>;
#endif
    %(op_src_decl)s;
    %(op_rd)s;

#if %(is_vecWhole)s
    bool vm_zero = false;
#else
    VM_REQUIRED();
    bool vm_zero = !this->vm && popcount_in_byte(tmp_v0.as<uint64_t>(), vmi.rs, vmi.re) == 0;
#endif

    Fault fault = NoFault;

    uint32_t eew_ = eew;
#if %(is_vecIndex)s
    eew_ = sizeof(vu) * 8;// eew_ = sew
    uint32_t vs2ElemIdx = vmi.rs % (VLEN / eew);
#endif
    uint32_t elem_num_per_vreg = VLEN / eew_;
    uint32_t eewb = eew_ / 8;

    Addr EA;
    %(ea_code)s;

    // mem_size = calc_memsize(vmi.rs, vmi.re, eew_, rVl);
    // if is vecWhole l/s mem_size = VLENB
    uint32_t mem_size = %(calc_memsize_code)s;
    mem_size = vm_zero ? 0 : mem_size;

    std::vector<bool> byte_enable(mem_size, true);

    %(vfof_set_code)s;
    %(vfof_zero_idx_check_code)s;

    fault = initiateMemRead(xc, EA, mem_size, memAccessFlags,
                                  byte_enable);
    return fault;
}

}};

def template VloadMicroCompleteAcc {{

#if %(is_vecIndex)s
template<typename ElemType>
Fault
%(class_name)s<ElemType>::completeAcc(PacketPtr pkt, ExecContext *xc,
                            Trace::InstRecord *traceData) const

#else

Fault
%(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
                            Trace::InstRecord *traceData) const

#endif
{
#if %(is_vecIndex)s
    using vu = std::make_unsigned_t<ElemType>;
#endif

    %(op_decl)s;
    %(op_rd)s;

#if %(is_vecWhole)s
    // VM_REQUIRED();
    // COPY_OLD_VD();
    bool vm_zero = false;
#else
    VM_REQUIRED();
    COPY_OLD_VD();
    bool vm_zero = !this->vm && popcount_in_byte(tmp_v0.as<uint64_t>(), vmi.rs, vmi.re) == 0;
#endif

    uint32_t eew_ = eew;
#if %(is_vecIndex)s
    eew_ = sizeof(vu) * 8;
#endif

    uint32_t elem_num_per_vreg = VLEN / eew_;
    uint32_t eewb = eew_ / 8;
    uint32_t mem_size = %(calc_memsize_code)s;
    mem_size = vm_zero ? 0 : mem_size;
    if (pkt) {
        memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());
        assert(mem_size == pkt->getSize());
    } else {
        assert(mem_size == 0);
    }

    %(divrVl)s;

    size_t ei;
    for (size_t i = 0; i < mem_size / eewb; i++) {
        uint32_t vdElemIdx = (vmi.rs % elem_num_per_vreg) + i;
        ei = i + vmi.rs;
        if (%(wb_elem_mask)s) {
            %(memacc_code)s;
        }
    }

    %(vfof_get_code)s;
    %(op_wb)s;
    return NoFault;
}

}};


def template VleffConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{

    const int32_t elem_num_per_vreg = VLEN / eew;
    const uint32_t num_microops = vflmul < 1 ? 1 : vflmul;

    StaticInstPtr microop;
    VectorMicroInfo vmi;
    for (int i = 0; i < num_microops; ++i) {
        vmi.rs = i * elem_num_per_vreg;
        vmi.re = (i + 1) * elem_num_per_vreg;
        vmi.microVd = VD + i;
        microop = new %(class_name)sMicro(_machInst, i, vmi);
        microop->setDelayedCommit();
        microop->setFlag(IsLoad);
        this->microops.push_back(microop);
    }
    microop = new VleffEndMicroInst(_machInst, num_microops);
    this->microops.push_back(microop);

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VleffMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[2];
    std::unique_ptr<size_t> fault_elem_idx;
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
        _numTypedDestRegs[VecRegClass]++;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, VecTempReg0 + _microIdx));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        SET_VL_SRC();
        SET_OLDDST_SRC();
        SET_VM_SRC();

        fault_elem_idx = std::make_unique<size_t>(0);
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;

};

}};

def template VleffMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    Addr EA;
    %(op_decl)s;
    %(op_rd)s;
    %(ea_code)s;

    VM_REQUIRED();
    COPY_OLD_VD();

    uint32_t elem_num_per_vreg = VLEN / eew;

    uint32_t mem_size = calc_memsize(microIdx * elem_num_per_vreg, (microIdx + 1) * elem_num_per_vreg, eew, rVl);

    const std::vector<bool> byte_enable(mem_size, true);
    Fault fault = xc->readMem(EA, Mem.as<uint8_t>(), mem_size, memAccessFlags,
                              byte_enable);
    if (mem_size > 0) {
        %(vfof_set_code)s;
        %(vfof_zero_idx_check_code)s;
        %(vfof_get_code)s;

        size_t elem_num_per_vreg = VLEN / width_EEW(machInst.width);

        size_t ei;
        for (size_t i = 0; i < std::min(elem_num_per_vreg, *fault_elem_idx); i++) {
            ei = i + elem_num_per_vreg * microIdx;
            %(memacc_code)s;
        }
    } else {
        %(vfof_get_code)s;
        assert(fault == NoFault);
    }

    %(op_wb)s;
    return fault;
}

}};


def template VlWholeConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    size_t NFIELDS = machInst.nf + 1;
    eew = 8;
    StaticInstPtr microop;
    VectorMicroInfo vmi;
    for (int i = 0; i < NFIELDS; ++i) {
        vmi.rs = 0;
        vmi.re = VLEN / eew;
        vmi.microVd = VD + i;
        microop = new %(class_name)sMicro(_machInst, i, vmi);
        microop->setDelayedCommit();
        microop->setFlag(IsLoad);
        this->microops.push_back(microop);
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VlWholeMicroDeclare {{

class %(class_name)s: public %(base_class)s
{
private:
    RegId destRegIdxArr[1];
    RegId srcRegIdxArr[8];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
        : %(base_class)s("%(mnemonic)s_micro", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        this->flags[IsVector] = true;
        this->flags[IsLoad] = true;
    }
    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                        Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VlWholeMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    Addr EA;
    %(op_decl)s;
    %(op_rd)s;
    %(ea_code)s;

    Fault fault = readMemAtomicLE(xc, traceData, EA,
                                  *(vreg_t::Container*)(&Mem), memAccessFlags);
    if (fault != NoFault)
        return fault;

    size_t elem_per_reg = VLEN / eew;
    for (size_t i = 0; i < elem_per_reg; i++) {
        %(memacc_code)s;
    }

    %(op_wb)s;
    return NoFault;
}

}};

def template VlStrideConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    const uint32_t elem_num_per_vreg = VLEN / eew;
    // panic_if(nf * emul > 8, "invalid nf and emul");


    StaticInstPtr microop;
    VectorMicroInfo vmi;
    uint32_t vlmax = VLEN / sew * vflmul;
    for (int i=0; i < vlmax; i++) {
        for (int fn=0; fn < nf; fn++) {
            // baseAddr = Rs1 + i*Rs2 + fn * eew / 8
            // mem<eew>[baseAddr] = (vd + fn * emul)<eew>[i]
            vmi.rs = i;
            vmi.re = i+1;
            vmi.fn = fn;
            vmi.microVd = elem_gen_idx(VD + fn * emul, i, eew/8);
            vmi.offset = fn * eew / 8;
            microop = new %(class_name)sMicro(_machInst, i*fn, vmi);
            microop->setDelayedCommit();
            microop->setFlag(IsLoad);

            microop->setNF(nf);
            if (nf > 1) {
                // segment vl related
                microop->resetOpClass(VectorSegStridedLoadOp);
                microop->setFlag(IsSegLoad);
            }

            this->microops.push_back(microop);
        }
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VlStrideMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    // rs1, rs2, vd, vm
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVd);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs2));
        SET_VL_SRC();
        SET_OLDDST_SRC();
        SET_VM_SRC();
        this->flags[IsLoad] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VlStrideMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;

    // EA = Rs1 + Rs2*vmi.rs + vmi.offset;
    %(ea_code)s;

    uint32_t elem_num_per_vreg = VLEN / eew;
    uint32_t mem_size = calc_memsize(vmi.rs, vmi.re, eew, rVl);

    VM_REQUIRED();

    COPY_OLD_VD();

    uint32_t vdElemIdx = (vmi.rs % elem_num_per_vreg);
    size_t ei = vmi.rs;
    if ((ei < rVl) && (machInst.vm || elem_mask(v0, ei))) {
        const std::vector<bool> byte_enable(mem_size, true);
        fault = xc->readMem(EA, Mem.as<uint8_t>(), mem_size,
                                memAccessFlags, byte_enable);
        if (fault != NoFault)
            return fault;
        %(memacc_code)s;
    }

    %(op_wb)s;
    return fault;
}

}};

def template VlIndexConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = width_EEW(_machInst.width); // vs2 : index[i]
    const uint32_t sew = getSew(_machInst.vtype8.vsew); // vs3 : stored elem
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    uint32_t flmul = vflmul < 1 ? 1 : vflmul;
    const uint32_t elem_num_per_vreg = VLEN / sew;
    // panic_if(nf * flmul > 8, "invalid nf and emul");

    StaticInstPtr microop;
    VectorMicroInfo vmi;
    uint32_t vlmax = VLEN / sew * vflmul;
    for (int i = 0; i < vlmax; i++) {
        for (int fn = 0; fn < nf; fn++) {
            // baseAddr = Rs1 + fn * sew / 8
            // (vd + fn * flmul)<sew>[i] = mem<sew>[baseAddr + vs2<eew>[i]]
            vmi.fn = fn;
            vmi.rs = i;
            vmi.re = i+1;
            vmi.microVd = elem_gen_idx(VD + fn * flmul, i, sew/8);
            vmi.microVs2 = elem_gen_idx(VS2, i, eew / 8);
            vmi.offset = fn * sew / 8;
            if (vmi.microVd >= 32) {
                break;
            }
            microop = new %(class_name)sMicro<ElemType>(machInst, i * fn, vmi);
            microop->setFlag(IsDelayedCommit);
            microop->setFlag(IsLoad);

            microop->setNF(nf);
            if (nf > 1) {
                // segment vl related
                microop->resetOpClass(VectorSegIndexedLoadOp);
                microop->setFlag(IsSegLoad);
            }

            this->microops.push_back(microop);
        }
    }

    this->microops.front()->setFlag(IsFirstMicroop);
    this->microops.back()->setFlag(IsLastMicroop);
    this->flags[IsVector] = true;
}

}};

def template VlIndexMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // rs1, vs2, vd, vm
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint32_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        vmi = _vmi;
        assert(~vmi.fn);
        assert(~vmi.microVd);
        assert(~vmi.microVs2);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs2));
        // We treat agnostic as undistrubed
        SET_VL_SRC();
        SET_OLDDST_SRC();
        SET_VM_SRC();
        this->flags[IsLoad] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VlIndexMicroExecute {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext *xc,
    Trace::InstRecord *traceData)const
{
    using vu = std::make_unsigned_t<ElemType>;
    %(op_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;
    uint32_t vs2ElemIdx = vmi.rs % (VLEN / width_EEW(machInst.width));
    %(ea_code)s;


    VM_REQUIRED();

    COPY_OLD_VD();

    constexpr uint8_t elem_size = sizeof(vu);
    uint32_t mem_size = elem_size;
    const std::vector<bool> byte_enable(mem_size, true);

    uint32_t vdElemIdx = vmi.rs % (VLENB / elem_size);
    size_t ei = vmi.rs;
    if (machInst.vm || elem_mask(v0, ei)) {
        fault = xc->readMem(EA, Mem.as<uint8_t>(), mem_size,
                                memAccessFlags, byte_enable);
        if (fault != NoFault)
            return fault;
        %(memacc_code)s;
    }

    %(op_wb)s;
    return fault;
}

}};

def template VseConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    const uint32_t elem_num_per_vreg = VLEN / eew;
    // panic_if(nf * emul > 8, "invalid nf and emul");

    StaticInstPtr microop;

    VectorMicroInfo vmi;
    if (nf == 1) {
        const uint32_t num_microops = emul;
        for (int i = 0; i < num_microops; ++i) {
            vmi.rs = i * elem_num_per_vreg;
            vmi.re = (i+1) * elem_num_per_vreg;
            vmi.microVs3 = VS3 + i;
            vmi.fn = 0;
            vmi.offset = (i * VLEN) / 8;
            microop = new %(class_name)sMicro(_machInst, i, vmi);
            microop->setDelayedCommit();
            microop->setFlag(IsStore);
            this->microops.push_back(microop);
        }
    } else {
        uint32_t vlmax = VLEN / sew * vflmul;
        for (int i=0; i < vlmax; i++) {
            for (int fn=0; fn < nf; fn++) {
                // baseAddr = Rs1 + (i*nf + fn) * eew / 8
                // mem<eew>[baseAddr] = (vs3 + fn * emul)<eew>[i];
                vmi.rs = i;
                vmi.re = i+1;
                vmi.fn = fn;
                vmi.microVs3 = elem_gen_idx(VS3 + fn * emul, i, eew/8);
                vmi.offset = (i*nf+fn) * eew / 8;
                microop = new %(class_name)sMicro(_machInst, i*(fn+1), vmi);
                microop->resetOpClass(VectorSegUnitStrideStoreOp);
                microop->setDelayedCommit();
                microop->setFlag(IsStore);
                this->microops.push_back(microop);
            }
        }
    }

    this->microops.front()->setFlag(IsFirstMicroop);
    this->microops.back()->setFlag(IsLastMicroop);
}

}};

def template VseMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[0];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
        : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVs3);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs3));
        SET_VL_SRC();
        SET_VM_SRC();
        this->flags[IsVector] = true;
        this->flags[IsStore] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VstoreMicroExecute {{

#if %(is_vecIndex)s

template<typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext *xc, Trace::InstRecord *traceData)const

#else

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const

#endif
{
#if %(is_vecIndex)s
    using vu = std::make_unsigned_t<ElemType>;
#endif

    %(op_decl)s;
    %(op_rd)s;

    Addr EA;

#if %(is_vecIndex)s
    uint32_t vs2ElemIdx = vmi.rs % (VLEN / eew);
#endif

    %(ea_code)s;

    uint32_t eew_ = eew;
#if %(is_vecIndex)s
    eew_ = sizeof(vu) * 8;
#endif
    const uint32_t eewb = eew_/8;
    %(divrVl)s;

    // calc_memsize(vmi.rs, vmi.re, eew_, rVl);
    uint32_t mem_size = %(calc_memsize_code)s;

#if %(is_vecWhole)s
#else
    VM_REQUIRED();
#endif

    std::vector<bool> byte_enable(mem_size, false);

    size_t ei;
    for (size_t i = 0; i < mem_size / eewb; i++) {
        uint32_t vs3ElemIdx = (vmi.rs % (VLEN / eew_)) + i;
        ei = i + vmi.rs;
        if (%(wb_elem_mask)s) {
            %(memacc_code)s;
            auto it = byte_enable.begin() + i * eewb;
            std::fill(it, it + eewb, true);
        }
    }

    Fault fault;
    fault = xc->writeMem(Mem.as<uint8_t>(), mem_size, EA, memAccessFlags,
                         nullptr, byte_enable);
    return fault;
}

}};

def template VstoreMicroInitiateAcc {{

#if %(is_vecIndex)s

template<typename ElemType>
Fault
%(class_name)s<ElemType>::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const

#else

Fault
%(class_name)s::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const

#endif
{
#if %(is_vecIndex)s
    using vu = std::make_unsigned_t<ElemType>;
#endif

    %(op_decl)s;
    %(op_rd)s;

    Addr EA;

#if %(is_vecIndex)s
    uint32_t vs2ElemIdx = vmi.rs % (VLEN / eew);
#endif

    %(ea_code)s;

    uint32_t eew_ = eew;
#if %(is_vecIndex)s
    eew_ = sizeof(vu) * 8;
#endif
    const uint32_t eewb = eew_/8;

    %(divrVl)s;

    // calc_memsize(vmi.rs, vmi.re, eew_, rVl);
    uint32_t mem_size = %(calc_memsize_code)s;

#if %(is_vecWhole)s
#else
    VM_REQUIRED();
#endif

    std::vector<bool> byte_enable(mem_size, false);

    size_t ei;
    for (size_t i = 0; i < mem_size / eewb; i++) {
        uint32_t vs3ElemIdx = (vmi.rs % (VLEN / eew_)) + i;
        ei = i + vmi.rs;
        if (%(wb_elem_mask)s) {
            %(memacc_code)s;
            auto it = byte_enable.begin() + i * eewb;
            std::fill(it, it + eewb, true);
        }
    }

    Fault fault = NoFault;
    if (mem_size > 0) {
        fault = xc->writeMem(Mem.as<uint8_t>(), mem_size, EA, memAccessFlags,
                            nullptr, byte_enable);
    }
    return fault;
}

}};

def template VstoreMicroCompleteAcc {{

#if %(is_vecIndex)s

template<typename ElemType>
Fault
%(class_name)s<ElemType>::completeAcc(PacketPtr pkt, ExecContext* xc,
                            Trace::InstRecord* traceData) const

#else

Fault
%(class_name)s::completeAcc(PacketPtr pkt, ExecContext* xc,
                            Trace::InstRecord* traceData) const

#endif
{
    return NoFault;
}

}};

def template VlmConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    eew = 8;
    _machInst.width = 0;
    const uint32_t emul = 1;
    const uint32_t elem_num_per_vreg = VLEN / eew;
    // panic_if(nf * emul > 8, "invalid nf and emul");

    uint32_t vlmax = VLENB;
    StaticInstPtr microop;

    VectorMicroInfo vmi;
    if (nf == 1) {
        vmi.rs = 0;
        vmi.re = vlmax;
        vmi.microVd = VD;
        vmi.offset = 0;
        microop = new Vlm_vMicro(_machInst, 0, vmi);
        microop->setDelayedCommit();
        microop->setFlag(IsLoad);
    } else {
        // TODO
        panic("not implemented");
    }
    this->microops.push_back(microop);

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};


def template VlmMicroDeclare {{

class %(class_name)s : public VleMicroInst
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
    : VleMicroInst("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVd);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        SET_VL_SRC();
        SET_OLDDST_SRC();
        SET_VM_SRC();

        this->flags[IsVector] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using VleMicroInst::generateDisassembly;

};

}};

def template VlmMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    Addr EA;
    // EA = Rs1 + vmi.offset;
    %(ea_code)s;

    uint32_t elem_num_per_vreg = VLEN / eew;
    uint32_t mem_size = calc_memsize(vmi.rs, vmi.re, eew, rVl);

    VM_REQUIRED();

    std::vector<bool> byte_enable(mem_size, true);
    Fault fault = xc->readMem(EA, Mem.as<uint8_t>(), mem_size, memAccessFlags,
                              byte_enable);
    if (fault != NoFault)
        return fault;

    COPY_OLD_VD();

    rVl = (rVl + 7) / 8;
    size_t ei;
    for (size_t i = 0; i < vmi.re - vmi.rs; i++) {
        uint32_t vdElemIdx = (vmi.rs % elem_num_per_vreg) + i;
        ei = i + vmi.rs;
        %(memacc_code)s;
    }

    %(op_wb)s;
    return fault;
}

}};

def template VsmConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    eew = 8;
    _machInst.width = 0;
    const uint32_t emul = 1;
    const uint32_t elem_num_per_vreg = VLEN / eew;
    // panic_if(nf * emul > 8, "invalid nf and emul");

    uint32_t vlmax = 16;
    StaticInstPtr microop;

    VectorMicroInfo vmi;
    if (nf == 1) {
        vmi.rs = 0;
        vmi.re = vlmax;
        vmi.microVs3 = VS3;
        vmi.offset = 0;
        microop = new Vsm_vMicro(_machInst, 0, vmi);
        microop->setDelayedCommit();
        microop->setFlag(IsStore);
    } else {
        // TODO
        panic("not implemented");
    }
    this->microops.push_back(microop);

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VsmMicroDeclare {{

class %(class_name)s : public VseMicroInst
{
private:
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[0];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
        : VseMicroInst("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVs3);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs3));
        SET_VL_SRC();
        SET_VM_SRC();
        this->flags[IsVector] = true;
        this->flags[IsStore] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using VseMicroInst::generateDisassembly;
};

}};

def template VsWholeConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
  : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    size_t NFIELDS = machInst.nf + 1;
    eew = 8;
    StaticInstPtr microop;
    VectorMicroInfo vmi;
    for (int i = 0; i < NFIELDS; ++i) {
        vmi.rs = 0;
        vmi.re = VLEN / eew;
        microop = new %(class_name)sMicro(_machInst, i, vmi);
        microop->setDelayedCommit();
        microop->setFlag(IsStore);
        this->microops.push_back(microop);
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VsWholeMicroDeclare {{

class %(class_name)s: public %(base_class)s
{
private:
    RegId destRegIdxArr[0];
    RegId srcRegIdxArr[8];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
        : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, _machInst.vs3 + _microIdx));
        this->flags[IsVector] = true;
        this->flags[IsStore] = true;
    }
    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                        Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VsStrideConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = width_EEW(_machInst.width);
    const uint32_t sew = getSew(_machInst.vtype8.vsew);
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    const uint32_t elem_num_per_vreg = VLEN / eew;
    // panic_if(nf * emul > 8, "invalid nf and emul");

    StaticInstPtr microop;
    VectorMicroInfo vmi;
    uint32_t vlmax = VLEN / sew * vflmul;
    for (int i=0; i < vlmax; i++) {
        for (int fn=0; fn < nf; fn++) {
            // baseAddr = Rs1 + i*Rs2 + fn * eew / 8
            // mem<eew>[baseAddr] = (vs3 + fn * emul)<eew>[i];
            vmi.rs = i;
            vmi.re = i+1;
            vmi.fn = fn;
            vmi.microVs3 = elem_gen_idx(VS3 + fn * emul, i, eew/8);
            vmi.offset = fn * eew / 8;
            microop = new %(class_name)sMicro(_machInst, i*fn, vmi);
            microop->setDelayedCommit();
            microop->setFlag(IsStore);
            this->microops.push_back(microop);
        }
    }

    this->microops.front()->setFlag(IsFirstMicroop);
    this->microops.back()->setFlag(IsLastMicroop);
}

}};

def template VsStrideMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    // rs1, rs2, vs3, vm
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[0];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
        : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s,  _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVs3);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs2));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs3));
        SET_VL_SRC();
        SET_VM_SRC();
        this->flags[IsStore] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VsIndexConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    uint32_t flmul = vflmul < 1 ? 1 : vflmul;
    const uint32_t elem_num_per_vreg = VLEN / sew;
    // panic_if(nf * flmul > 8, "invalid nf and emul");

    StaticInstPtr microop;
    VectorMicroInfo vmi;
    uint32_t vlmax = VLEN / sew * vflmul;
    for (int i=0; i < vlmax; i++) {
        // can be optimized
        for (int fn=0; fn<nf; fn++) {
            // baseAddr = Rs1 + fn * sew / 8
            // mem<sew>[baseAddr + vs2<eew>[i]] = (vs3 + fn * flmul)<sew>[i]
            vmi.fn = fn;
            vmi.rs = i;
            vmi.re = i+1;
            vmi.microVs2 = elem_gen_idx(VS2, i, eew / 8);
            vmi.microVs3 = elem_gen_idx(VS3 + fn * flmul, i, sew / 8);
            vmi.offset = fn * sew / 8;
            if (vmi.microVs2 >= 32 || vmi.microVs3 >= 32) {
                break;
            }
            microop = new %(class_name)sMicro<ElemType>(machInst, i * fn, vmi);
            microop->setFlag(IsDelayedCommit);
            microop->setFlag(IsStore);
            this->microops.push_back(microop);
        }
    }
    this->microops.front()->setFlag(IsFirstMicroop);
    this->microops.back()->setFlag(IsLastMicroop);
    this->flags[IsVector] = true;
}

}};

def template VsIndexMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // rs1, vs2, vs3, vm
    RegId srcRegIdxArr[8];
    RegId destRegIdxArr[0];
public:
    %(class_name)s(ExtMachInst _machInst, uint32_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.fn);
        assert(~vmi.microVs2);
        assert(~vmi.microVs3);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs2));
        // We treat agnostic as undistrubed
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs3));
        SET_VL_SRC();
        SET_VM_SRC();
        this->flags[IsStore] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VMemTemplateDecodeBlock {{

switch(machInst.vtype8.vsew) {
    case 0b000: {
        return new %(class_name)s<uint8_t>(machInst);
    }
    case 0b001: {
        return new %(class_name)s<uint16_t>(machInst);
    }
    case 0b010: {
        return new %(class_name)s<uint32_t>(machInst);
    }
    case 0b011: {
        return new %(class_name)s<uint64_t>(machInst);
    }
    default: GEM5_UNREACHABLE;
}

}};

def template VleffDecodeBlock {{
    return new %(class_name)s(machInst);
}};
